{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will use torch_geometric to predict the developpement of a graph of positions through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'pycapmd'\n",
      "Cannot import simulator\n",
      "Using :  431\n",
      "Available :  3585\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from genericpath import exists\n",
    "\n",
    "import random\n",
    "\n",
    "sys.path.append('/home/nstillman/1_sbi_activematter/cpp_model')\n",
    "import allium\n",
    "\n",
    "model_path = \"model.pkl\"\n",
    "\n",
    "from cell_dataset import CellGraphDataset\n",
    "from cell_model import GraphEvolution\n",
    "\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes \n",
    "print(\"Available : \", process.memory_info().vms  // 1000000)  # in megabytes \n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a graph of cells having their own positions and velocity.\n",
    "\n",
    "In the graph, we will first start by connecting all the edges, then maybe later make radius_graphs to reduce the cost of the pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length :  200\n",
      "Test data length :  50\n",
      "Validation data length :  50\n"
     ]
    }
   ],
   "source": [
    "#path = \"data/\" #local\n",
    "path = \"/scratch/users/nstillman/data-cpp/\" #remote\n",
    "\n",
    "data_train = CellGraphDataset(root=path + 'train', max_size=200, rdts=True, inmemory=True, bg_load=True)\n",
    "print(\"Training data length : \", data_train.len())\n",
    "\n",
    "data_test = CellGraphDataset(root=path + 'test', max_size=50, inmemory=True, bg_load=True)\n",
    "print(\"Test data length : \", data_test.len())\n",
    " \n",
    "data_val = CellGraphDataset(root=path + 'valid', max_size=50, inmemory=True, bg_load=True)\n",
    "print(\"Validation data length : \", data_val.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sources\" not in os.listdir():\n",
    "    os.mkdir(\"sources\")\n",
    "    \n",
    "override = True #make this true to always use the same ones\n",
    "if override :\n",
    "    if \"train_paths.pkl\" not in os.listdir(\"sources\"):\n",
    "        #first time running, dump the paths to a pickle file\n",
    "        data_train.dump_source(\"sources/train_paths.pkl\")\n",
    "    else :\n",
    "        #overwrite the paths to the previous configuration\n",
    "        data_train.overwrite_source(\"sources/train_paths.pkl\")\n",
    "\n",
    "    if \"test_paths.pkl\" not in os.listdir(\"sources\"):\n",
    "        data_test.dump_source(\"sources/test_paths.pkl\")\n",
    "    else :\n",
    "        data_test.overwrite_source(\"sources/test_paths.pkl\")\n",
    "\n",
    "    if \"val_paths.pkl\" not in os.listdir(\"sources\"):\n",
    "        data_val.dump_source(\"sources/val_paths.pkl\")\n",
    "    else :\n",
    "        data_val.overwrite_source(\"sources/val_paths.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rval, edge_index, edge_attr, batch_edge = data_train.get(0)\n",
    "#print(rval, edge_index, edges_attr, batch_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the model that will be used :\n",
    "    > input \n",
    "        (1) Graph at a particular time t (nodes having x,y,dx,dy as attributes)\n",
    "        (2) Graphs up to a particular time [t-a, t] (nodes having x,y as attributes)\n",
    "    > output\n",
    "        (a) Graph at the immediate next time step t+1\n",
    "        (b) Graph [t, t+b]\n",
    "        (c) Graph at t+b\n",
    "    > graph size\n",
    "        (x) Fixed graph size to the most nodes possible (or above)\n",
    "        (y) Unbounded graph size\n",
    "            >> idea : graph walks\n",
    "            >> idea : sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model will do (1ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global losses\n",
    "losses = []\n",
    "\n",
    "def run_single(data, i, device) :\n",
    "    x, edge_index, edge_attr, batch_edges = data.get(i)\n",
    "\n",
    "    xshape = x.shape\n",
    "\n",
    "    x = x.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "    edge_attr = edge_attr.to(device)\n",
    "    batch_edges = batch_edges.to(device)\n",
    "\n",
    "    edge_index = edge_index - batch_edges * xshape[1]\n",
    "\n",
    "    #we don't want to predict the last step since we wouldn't have the data for the loss\n",
    "    #and for the first point we don't have the velocity\n",
    "    mask = torch.logical_and(batch_edges > 0, \\\n",
    "                             batch_edges < xshape[0] - 1)\n",
    "    out = model(x[1:-1], edge_index[:, mask] , edge_attr[mask])\n",
    "    \n",
    "    loss = F.mse_loss(out, x[2:])\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, device) :\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        for i in range(data.len()):\n",
    "            loss = run_single(data, i, device)\n",
    "            \n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            \n",
    "        return loss_sum / data.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, data, device, epoch) :\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    for i in range(data.len()):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = run_single(data, i, device)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step(epoch + i / data.len())\n",
    "        \n",
    "        print(\"Current loss : \" , loss.item(), \" ... \", i, \"/\", data.len(), \". Current memory usage : \", process.memory_info().rss // 1000000, \" MB, loaded \", len(data.memory), \"    \", end=\"\\r\")  # in megabytes \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(model, optimizer, scheduler, data_train, data_test, device, epoch, offset, save=0, early_stop=False):\n",
    "    for e in range(offset, offset + epoch):\n",
    "        model = train(model, optimizer, scheduler, data_train, device, e)\n",
    "            \n",
    "        test_loss = test(model, data_test, device)\n",
    "        \n",
    "        print(\"Epoch : \", e, \"Test loss : \", test_loss, \"                                                         \")\n",
    "        \n",
    "        losses.append(test_loss)\n",
    "        \n",
    "        if early_stop and len(losses) > 30 :\n",
    "            recent_losses = min(len(losses), 30)\n",
    "            y = losses[-recent_losses:]\n",
    "            \n",
    "            axis = np.arange(recent_losses)\n",
    "            A = np.vstack([axis, np.ones(len(axis))]).T\n",
    "            \n",
    "            a = np.linalg.lstsq(A, y, rcond=None)\n",
    "            \n",
    "            if a[0][0] >= -0.002 :\n",
    "                print(\"Early stopping : recent slope at \", a[0][0])\n",
    "                if (save) :\n",
    "                    with open(model_path, 'wb') as f:\n",
    "                        pickle.dump(model, f)\n",
    "                return\n",
    "            else : \n",
    "                print(\"Early stopping passed : current slope at \", a[0][0])\n",
    "        \n",
    "        if (save and (e%save == 0 or e == epoch-1)) :\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "if (load and exists(model_path)) :\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "else :\n",
    "    model = GraphEvolution(4, 4, 32, 8, 0.0, edge_dim=len(data_train.attributes));\n",
    "    \n",
    "assert isinstance(model, GraphEvolution)\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphingLoss():\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.stop = False\n",
    "        self.timer = 0\n",
    "        self.last_len = 0\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "\n",
    "    def plot_and_reschedule(self):\n",
    "        if not self.stop:\n",
    "            if (self.last_len != len(losses)) :\n",
    "                if self.fig is None :\n",
    "                    self.fig = plt.figure()\n",
    "                    self.ax = self.fig.add_subplot(111)\n",
    "                \n",
    "                self.ax.clear() # type: ignore\n",
    "                self.ax.plot(losses) # type: ignore\n",
    "                self.fig.show()\n",
    "                self.fig.canvas.draw()\n",
    "                self.last_len = len(losses)\n",
    "                \n",
    "                plt.savefig(\"Losses.pdf\", format=\"pdf\")\n",
    "\n",
    "            threading.Timer(self.timer, self.plot_and_reschedule).start()\n",
    "            \n",
    "    def gstop(self):\n",
    "        self.stop = True\n",
    "        \n",
    "    def gstart(self, timer=20):\n",
    "        self.timer = timer\n",
    "        if (not self.timer or self.timer != int(self.timer)):\n",
    "            raise ValueError(\"timer must be a positive integer\")\n",
    "        \n",
    "        threading.Timer(self.timer, self.plot_and_reschedule).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  501 Test loss :  1.6410113921761513                                                                \n",
      "Current loss :  2.792478561401367  ...  145 / 200 . Current memory usage :  9055  MB, bg_loading  -2       \r"
     ]
    }
   ],
   "source": [
    "epochs = 501\n",
    "grapher = GraphingLoss()\n",
    "try :\n",
    "    grapher.gstart(20)\n",
    "    start(model, optimizer, scheduler, data_train, data_test, \"cuda\" if torch.cuda.is_available() else \"cpu\", epochs, len(losses), save=50, early_stop = False)\n",
    "finally :\n",
    "    grapher.gstop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  9055\n",
      "Available :  28099\n"
     ]
    }
   ],
   "source": [
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes \n",
    "print(\"Available : \", process.memory_info().vms  // 1000000)  # in megabytes \n",
    "\n",
    "#things to do :\n",
    "    #normalize the inputs \n",
    "    #make sure this model doesn't use past times (normally should not since the graph is disconnected and it's based on message passing)\n",
    "    #show the results\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geom]",
   "language": "python",
   "name": "conda-env-geom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c671acb9d3cf87d64ea7277f1f1b7981cda534cb27553b34f82b93fda7c98f17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
