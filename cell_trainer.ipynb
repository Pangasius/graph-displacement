{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will use torch_geometric to predict the developpement of a graph of positions through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI realized I am leaning towards this approach https://doi.org/10.1016/j.trc.2020.102635\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I realized I am leaning towards this approach https://doi.org/10.1016/j.trc.2020.102635\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gille\\miniconda3\\envs\\geom\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  291\n",
      "Available :  3271\n",
      "True\n",
      "Could not import allium\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from genericpath import exists\n",
    "\n",
    "model_path = \"models/model\"\n",
    "loss_path = \"models/loss\"\n",
    "\n",
    "from cell_dataset import CellGraphDataset, extract_train_test_val\n",
    "from cell_model import GraphEvolution\n",
    "from cell_utils import GraphingLoss\n",
    "from cell_training import train, test_single, test_recursive, compute_parameters\n",
    "\n",
    "import os, psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes \n",
    "print(\"Available : \", process.memory_info().vms  // 1000000)  # in megabytes \n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "#https://github.com/clovaai/AdamP\n",
    "from adamp import AdamP\n",
    "\n",
    "sys.path.append('/home/nstillman/1_sbi_activematter/cpp_model')\n",
    "try :\n",
    "    import allium\n",
    "except :\n",
    "    print(\"Could not import allium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a graph of cells having their own positions and velocity.\n",
    "\n",
    "In the graph, we will first start by connecting all the edges, then maybe later make radius_graphs to reduce the cost of the pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all =  True#load directly from a pickle\n",
    "pre_separated = False #if three subfolders already exist for train test and val\n",
    "\n",
    "override = False #make this true to always use the same ones\n",
    "\n",
    "if load_all : \n",
    "    if os.path.exists(\"data/training.pkl\") :\n",
    "        with open(\"data/training.pkl\", \"rb\") as f:\n",
    "            data_train = pickle.load(f)\n",
    "    if os.path.exists(\"data/testing.pkl\") :\n",
    "        with open(\"data/testing.pkl\", \"rb\") as f:\n",
    "            data_test = pickle.load(f)\n",
    "else :\n",
    "    #path = \"data/\" #local\n",
    "    \n",
    "    if pre_separated :\n",
    "        path = \"/scratch/users/nstillman/data-cpp/\" #remote for wrapped\n",
    "        \n",
    "        data_train = CellGraphDataset(root=path + 'train', max_size=1000, inmemory=True, bg_load=True, wrap=True, T_limit=16)\n",
    "        print(\"Training data length : \", data_train.len())\n",
    "\n",
    "        data_test = CellGraphDataset(root=path + 'test', max_size=50, inmemory=True, bg_load=True, wrap=True, T_limit=16)\n",
    "        print(\"Test data length : \", data_test.len())\n",
    "        \n",
    "        data_val = CellGraphDataset(root=path + 'valid', max_size=50, inmemory=True, bg_load=True, wrap=True, T_limit=8)\n",
    "        print(\"Validation data length : \", data_val.len())\n",
    "    else :\n",
    "        path = \"/scratch/users/nstillman/open/low_tau_high_v0/\"\n",
    "        \n",
    "        data_train, data_test, data_val =  extract_train_test_val(path, max_size=1000, inmemory=True, bg_load=True, wrap=False, T_limit=0)\n",
    "\n",
    "    if override :\n",
    "        data_train.save_or_load_if_exists(\"train_paths.pkl\")\n",
    "        data_test.save_or_load_if_exists(\"test_paths.pkl\")\n",
    "        data_val.save_or_load_if_exists(\"val_paths.pkl\")\n",
    "    else :\n",
    "        torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO : if bg_load is True, this starts the loading, if skipped, bg_loading will take place as soon as a get is called\n",
    "rval, edge_index, edge_attr, batch_edge, border, params = data_train.get(0)\n",
    "rval, edge_index, edge_attr, batch_edge, border, params = data_test.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the model that will be used :\n",
    "    > input \n",
    "        (1) Graph at a particular time t (nodes having x,y,dx,dy as attributes)\n",
    "        (2) Graphs up to a particular time [t-a, t] (nodes having x,y as attributes)\n",
    "    > output\n",
    "        (a) Graph at the immediate next time step t+1\n",
    "        (b) Graph [t, t+b]\n",
    "        (c) Graph at t+b\n",
    "    > graph size\n",
    "        (x) Fixed graph size to the most nodes possible (or above)\n",
    "        (y) Unbounded graph size\n",
    "            >> idea : graph walks\n",
    "            >> idea : sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model will do (1ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(model : GraphEvolution, optimizer : torch.optim.Optimizer, scheduler  : torch.optim.lr_scheduler._LRScheduler,\\\n",
    "          data_train : CellGraphDataset, data_test : CellGraphDataset, device : torch.device, epoch : int, offset : int, grapher : GraphingLoss, save=0, save_datasets=True):\n",
    "    for e in range(offset, offset + epoch):\n",
    "        \n",
    "        recursive = e > 20\n",
    "\n",
    "        model = train(model, optimizer, scheduler, data_train, device, e, process, max_epoch=offset+epoch, recursive=recursive)\n",
    "\n",
    "        #model.show_gradients()\n",
    "        \n",
    "        if(e == 0 and save_datasets) :\n",
    "            data_train.thread = None\n",
    "            data_test.thread = None\n",
    "            with open(\"data/training.pkl\", 'wb') as f:\n",
    "                pickle.dump(data_train, f)\n",
    "            with open(\"data/testing.pkl\", 'wb') as f:\n",
    "                pickle.dump(data_test, f)\n",
    "            print(\"Saved datasets\")\n",
    "        \n",
    "\n",
    "        test_loss_s = test_single(model, data_test, device, duration=8)\n",
    "        test_loss_r = test_recursive(model, data_test, device, duration=8)\n",
    "\n",
    "        print(\"Epoch : \", e, \"Test loss : \", test_loss_s, \"Test loss recursive : \", test_loss_r)\n",
    "\n",
    "\n",
    "\n",
    "        grapher.losses.append(test_loss_r)\n",
    "        grapher.losses.append(test_loss_s)\n",
    "\n",
    "        grapher.plot_losses()\n",
    "        \n",
    "        if (e%10 == 0) :      \n",
    "            all_params_out, all_params_true = compute_parameters(model, data_test, device, duration=8)\n",
    "            grapher.plot_params(all_params_out, all_params_true, e)\n",
    "        \n",
    "        if (save and (e%save == 0 or e == epoch-1)) :\n",
    "            torch.save(model.state_dict(), model_path + str(e) + \".pt\")\n",
    "            with open(loss_path + str(e) + \".pkl\", 'wb') as f:\n",
    "                pickle.dump(grapher.losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "\n",
    "epoch_to_load = 10\n",
    "\n",
    "#model = GraphEvolution(in_channels=14, out_channels=4, hidden_channels=32, dropout=0.01, edge_dim=2, messages=5, wrap=data_train.wrap)\n",
    "model = GraphEvolution(in_channels=9, out_channels=4, hidden_channels=32, dropout=0.01, edge_dim=2, messages=5, wrap=True)\n",
    "losses = []\n",
    "\n",
    "if exists(model_path + str(epoch_to_load) + \".pt\") and load :\n",
    "    with open(loss_path + str(epoch_to_load) + \".pkl\", 'rb') as f:\n",
    "        losses = pickle.load(f)\n",
    "    model.load_state_dict(torch.load(model_path + str(epoch_to_load) + \".pt\"))\n",
    "    print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  2057\n",
      "Losses :  11\n",
      "Model :  GraphEvolution(\n",
      "  (encoder_resize): Linear(in_features=9, out_features=32, bias=True)\n",
      "  (encoder_resize2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gat_resize): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (gatv2s): ModuleList(\n",
      "    (0): GATv2Conv(256, 32, heads=8)\n",
      "    (1): GATv2Conv(256, 32, heads=8)\n",
      "    (2): GATv2Conv(256, 32, heads=8)\n",
      "    (3): GATv2Conv(256, 32, heads=8)\n",
      "    (4): GATv2Conv(256, 32, heads=8)\n",
      "  )\n",
      "  (gat_resize2): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "        (dropout3): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "        (dropout3): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_resize): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (decoder_resize2): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes\n",
    "print(\"Losses : \", len(losses) // 2)\n",
    "print(\"Model : \", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#might want to investigate AdamP \n",
    "optimizer = AdamP(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=5e-3, delta=0.1, wd_ratio=0.1, nesterov=True)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10, T_mult=2, eta_min=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  11 Test loss :  -3.9747566509246828 Test loss recursive :  0.006973451773228589\n",
      "Epoch :  12 Test loss :  -3.967327284812927 Test loss recursive :  0.006460898138466291\n",
      "Epoch :  13 Test loss :  -3.9766633033752443 Test loss recursive :  0.0071276993822539225\n",
      "Epoch :  14 Test loss :  -4.033767199516296 Test loss recursive :  0.006923718833568273\n",
      "Epoch :  15 Test loss :  -4.048538756370545 Test loss recursive :  0.006426337172742933\n",
      "Epoch :  16 Test loss :  -4.0207310962677 Test loss recursive :  0.008169496958144009 \n",
      "Epoch :  17 Test loss :  -3.9269494152069093 Test loss recursive :  0.009527474379865453\n",
      "Epoch :  18 Test loss :  -3.9880781412124633 Test loss recursive :  0.007405212555459002\n",
      "Epoch :  19 Test loss :  -3.981392741203308 Test loss recursive :  0.00624974496269715\n",
      "Current probability of recursive training :  0\n",
      "Epoch :  20 Test loss :  -4.054775857925415 Test loss recursive :  0.00815560057759285\n",
      "Epoch :  21 Test loss :  -3.9723953199386597 Test loss recursive :  0.0072960069286637005\n",
      "Epoch :  22 Test loss :  -3.976340193748474 Test loss recursive :  0.006717917614150792\n",
      "Epoch :  23 Test loss :  -4.055765862464905 Test loss recursive :  0.007183056090725586\n",
      "Epoch :  24 Test loss :  -3.97069064617157 Test loss recursive :  0.0064506855234503745\n",
      "Epoch :  25 Test loss :  -3.9618123245239256 Test loss recursive :  0.005648550063306175\n",
      "Epoch :  26 Test loss :  -3.967694125175476 Test loss recursive :  0.006453364200424403\n",
      "Epoch :  27 Test loss :  -3.9739714241027833 Test loss recursive :  0.005602348694810644\n",
      "Epoch :  28 Test loss :  -3.9615104007720947 Test loss recursive :  0.006503163407323882\n",
      "Epoch :  29 Test loss :  -3.96973961353302 Test loss recursive :  0.006509826221736148\n",
      "Current probability of recursive training :  tensor(0.2782)\n",
      "Epoch :  30 Test loss :  -3.9703832721710204 Test loss recursive :  0.005890799465996679\n",
      "Epoch :  31 Test loss :  -4.014763159751892 Test loss recursive :  0.008319129762239754\n",
      "Epoch :  32 Test loss :  -4.024841251373291 Test loss recursive :  0.006729028651025146\n",
      "Epoch :  33 Test loss :  -3.889494957923889 Test loss recursive :  0.007572690360248089\n",
      "Epoch :  34 Test loss :  -3.934454417228699 Test loss recursive :  0.007567875620443374\n",
      "Epoch :  35 Test loss :  -3.9807932996749877 Test loss recursive :  0.005437859279627446\n",
      "Epoch :  36 Test loss :  -4.049067435264587 Test loss recursive :  0.006599690424627625\n",
      "Epoch :  37 Test loss :  -3.9764386081695555 Test loss recursive :  0.006395791462127817\n",
      "Epoch :  38 Test loss :  -3.976328482627869 Test loss recursive :  0.0064760847848083355\n",
      "Epoch :  39 Test loss :  -3.896751012802124 Test loss recursive :  0.010695186948869378\n",
      "Current probability of recursive training :  tensor(0.2814)\n",
      "Epoch :  40 Test loss :  -4.069568791389465 Test loss recursive :  0.007181259754579514\n",
      "Epoch :  41 Test loss :  -3.9268777799606323 Test loss recursive :  0.011083792878780514\n",
      "Epoch :  42 Test loss :  -4.057213134765625 Test loss recursive :  0.006767886800225824\n",
      "Epoch :  43 Test loss :  -3.9344399118423463 Test loss recursive :  0.005496968185761944\n",
      "Epoch :  44 Test loss :  -4.033186130523681 Test loss recursive :  0.008985447278246284\n",
      "Epoch :  45 Test loss :  -4.050145778656006 Test loss recursive :  0.007321565953607206\n",
      "Epoch :  46 Test loss :  -4.058390026092529 Test loss recursive :  0.005762480237754062\n",
      "Epoch :  47 Test loss :  -3.982001633644104 Test loss recursive :  0.005378619031980634\n",
      "Epoch :  48 Test loss :  -3.9529636240005495 Test loss recursive :  0.00642194219166413\n",
      "Epoch :  49 Test loss :  -3.98186918258667 Test loss recursive :  0.006046410446288064\n",
      "Current probability of recursive training :  tensor(0.2846)\n",
      "Epoch :  50 Test loss :  -4.067631130218506 Test loss recursive :  0.00731982070603408\n",
      "Epoch :  51 Test loss :  -4.0761326837539675 Test loss recursive :  0.006688953255070374\n",
      "Epoch :  52 Test loss :  -4.049479885101318 Test loss recursive :  0.00821039744419977\n",
      "Epoch :  53 Test loss :  -4.0205381393432615 Test loss recursive :  0.008825132260099053\n",
      "Epoch :  54 Test loss :  -4.082261400222778 Test loss recursive :  0.005924156719993334\n",
      "Epoch :  55 Test loss :  -4.069283766746521 Test loss recursive :  0.008866984639316797\n",
      "Epoch :  56 Test loss :  -4.055642113685608 Test loss recursive :  0.00795682485215366\n",
      "Epoch :  57 Test loss :  -4.0867498254776 Test loss recursive :  0.006329850149923004 \n",
      "Epoch :  58 Test loss :  -3.985182857513428 Test loss recursive :  0.005374198724821326\n",
      "Epoch :  59 Test loss :  -3.9614505529403687 Test loss recursive :  0.006421813643537461\n",
      "Current probability of recursive training :  tensor(0.2877)\n",
      "Epoch :  60 Test loss :  -4.064157247543335 Test loss recursive :  0.006816590485395863\n",
      "Epoch :  61 Test loss :  -4.070129923820495 Test loss recursive :  0.007098156822321471\n",
      "Epoch :  62 Test loss :  -4.058148036003113 Test loss recursive :  0.008778697336092591\n",
      "Epoch :  63 Test loss :  -4.095630798339844 Test loss recursive :  0.007185961339855567\n",
      "Epoch :  64 Test loss :  -4.0307707023620605 Test loss recursive :  0.008422117119189353\n",
      "Epoch :  65 Test loss :  -4.056671848297119 Test loss recursive :  0.007313354139369039\n",
      "Epoch :  66 Test loss :  -4.062767767906189 Test loss recursive :  0.008571748989634215\n",
      "Epoch :  67 Test loss :  -4.089324288368225 Test loss recursive :  0.007681442964822054\n",
      "Epoch :  68 Test loss :  -4.054867291450501 Test loss recursive :  0.00852041961159557\n",
      "Epoch :  69 Test loss :  -4.09402494430542 Test loss recursive :  0.00807809651218122 \n",
      "Current probability of recursive training :  tensor(0.2909)\n",
      "Epoch :  70 Test loss :  -4.09799635887146 Test loss recursive :  0.006997256345930509\n",
      "Epoch :  71 Test loss :  -3.973207368850708 Test loss recursive :  0.005541751621640287\n",
      "Epoch :  72 Test loss :  -4.051800312995911 Test loss recursive :  0.007644348241738044\n",
      "Epoch :  73 Test loss :  -4.037986574172973 Test loss recursive :  0.008454683483578265\n",
      "Epoch :  74 Test loss :  -3.9959754848480227 Test loss recursive :  0.006193981960823294\n",
      "Epoch :  75 Test loss :  -3.9562233829498292 Test loss recursive :  0.011235818015411497\n",
      "Epoch :  76 Test loss :  -3.950959906578064 Test loss recursive :  0.007338716840022244\n",
      "Epoch :  77 Test loss :  -4.085194501876831 Test loss recursive :  0.005112842038215604\n",
      "Epoch :  78 Test loss :  -4.061468572616577 Test loss recursive :  0.00893264668971824\n",
      "Epoch :  79 Test loss :  -4.07628698348999 Test loss recursive :  0.007727674969937652\n",
      "Current probability of recursive training :  tensor(0.2942)\n",
      "Epoch :  80 Test loss :  -4.0898222160339355 Test loss recursive :  0.007324549003387801\n",
      "Epoch :  81 Test loss :  -4.08591164112091 Test loss recursive :  0.007821019212715328\n",
      "Epoch :  82 Test loss :  -4.070323977470398 Test loss recursive :  0.008373711932217703\n",
      "Epoch :  83 Test loss :  -4.080892324447632 Test loss recursive :  0.008116558231413364\n",
      "Epoch :  84 Test loss :  -4.091826291084289 Test loss recursive :  0.006998138594208285\n",
      "Epoch :  85 Test loss :  -4.095085334777832 Test loss recursive :  0.0070770311132946516\n",
      "Epoch :  86 Test loss :  -4.095716795921326 Test loss recursive :  0.006258310151752084\n",
      "Epoch :  87 Test loss :  -4.052960276603699 Test loss recursive :  0.008979023057036102\n",
      "Epoch :  88 Test loss :  -4.08612850189209 Test loss recursive :  0.006126368376426399\n",
      "Epoch :  89 Test loss :  -4.052448539733887 Test loss recursive :  0.009021839597844519\n",
      "Current probability of recursive training :  tensor(0.2974)\n",
      "Epoch :  90 Test loss :  -4.033324279785156 Test loss recursive :  0.010346824042499066\n",
      "Epoch :  91 Test loss :  -3.978956561088562 Test loss recursive :  0.008890933678485454\n",
      "Epoch :  92 Test loss :  -4.0949167108535764 Test loss recursive :  0.006949979045893997\n",
      "Epoch :  93 Test loss :  -4.024445366859436 Test loss recursive :  0.008538240795023739\n",
      "Epoch :  94 Test loss :  -4.085635657310486 Test loss recursive :  0.007409371710382402\n",
      "Epoch :  95 Test loss :  -4.063434133529663 Test loss recursive :  0.008365633245557547\n",
      "Epoch :  96 Test loss :  -4.105608072280884 Test loss recursive :  0.006126048963051289\n",
      "Epoch :  97 Test loss :  -4.0536909532547 Test loss recursive :  0.008721500192768872 \n",
      "Epoch :  98 Test loss :  -4.101297521591187 Test loss recursive :  0.005794405263150111\n",
      "Epoch :  99 Test loss :  -4.113720197677612 Test loss recursive :  0.005735515314154327\n",
      "Current probability of recursive training :  tensor(0.3007)\n",
      "Epoch :  100 Test loss :  -4.082179460525513 Test loss recursive :  0.006005262336984742\n",
      "Epoch :  101 Test loss :  -3.9568480920791624 Test loss recursive :  0.005756032423814759\n",
      "Epoch :  102 Test loss :  -3.981913194656372 Test loss recursive :  0.005542468288913369\n",
      "Epoch :  103 Test loss :  -4.069491019248963 Test loss recursive :  0.0050696638704539505\n",
      "Epoch :  104 Test loss :  -3.9731762409210205 Test loss recursive :  0.00459860975301126\n",
      "Epoch :  105 Test loss :  -4.018848757743836 Test loss recursive :  0.007901266561821102\n",
      "Epoch :  106 Test loss :  -4.055528855323791 Test loss recursive :  0.00577527899411507\n",
      "Epoch :  107 Test loss :  -4.055298953056336 Test loss recursive :  0.008403483517467976\n",
      "Epoch :  108 Test loss :  -4.082141904830933 Test loss recursive :  0.006428363230079412\n",
      "Epoch :  109 Test loss :  -4.073126029968262 Test loss recursive :  0.008054586870130151\n",
      "Current probability of recursive training :  tensor(0.3040)\n",
      "Epoch :  110 Test loss :  -4.08360643863678 Test loss recursive :  0.007707273783162236\n",
      "Epoch :  111 Test loss :  -4.079548778533936 Test loss recursive :  0.007811841000802815\n",
      "Epoch :  112 Test loss :  -4.090732107162475 Test loss recursive :  0.007014252204098739\n",
      "Epoch :  113 Test loss :  -4.092729229927063 Test loss recursive :  0.005599892118916614\n",
      "Epoch :  114 Test loss :  -3.99961332321167 Test loss recursive :  0.007648634290089831\n",
      "Epoch :  115 Test loss :  -3.9812620162963865 Test loss recursive :  0.004390742399264127\n",
      "Epoch :  116 Test loss :  -3.9693361377716063 Test loss recursive :  0.007517531569174025\n",
      "Epoch :  117 Test loss :  -4.070608983039856 Test loss recursive :  0.0075358132249675695\n",
      "Epoch :  118 Test loss :  -3.9863479566574096 Test loss recursive :  0.009029029125813395\n",
      "Epoch :  119 Test loss :  -3.983771352767944 Test loss recursive :  0.0055722732108552005\n",
      "Current probability of recursive training :  tensor(0.3073)\n",
      "Epoch :  120 Test loss :  -4.042434773445129 Test loss recursive :  0.00815113191260025\n",
      "Epoch :  121 Test loss :  -4.049911141395569 Test loss recursive :  0.008875362486578523\n",
      "Epoch :  122 Test loss :  -4.04300094127655 Test loss recursive :  0.008109294180758297\n",
      "Epoch :  123 Test loss :  -4.087181959152222 Test loss recursive :  0.005638172420440242\n",
      "Epoch :  124 Test loss :  -4.079713406562806 Test loss recursive :  0.006796890853438526\n",
      "Epoch :  125 Test loss :  -4.086663494110107 Test loss recursive :  0.0066200200136518105\n",
      "Epoch :  126 Test loss :  -4.06998236656189 Test loss recursive :  0.0076380845997482535\n",
      "Epoch :  127 Test loss :  -4.0723524141311644 Test loss recursive :  0.00712791868543718\n",
      "Epoch :  128 Test loss :  -4.085631308555603 Test loss recursive :  0.0075869756052270535\n",
      "Epoch :  129 Test loss :  -4.093340692520141 Test loss recursive :  0.006745346647221595\n",
      "Current probability of recursive training :  tensor(0.3106)\n",
      "Epoch :  130 Test loss :  -4.090984649658203 Test loss recursive :  0.006513538460130803\n",
      "Epoch :  131 Test loss :  -4.092072782516479 Test loss recursive :  0.006680718888528645\n",
      "Epoch :  132 Test loss :  -4.093991031646729 Test loss recursive :  0.006383456973126158\n",
      "Epoch :  133 Test loss :  -4.088623600006104 Test loss recursive :  0.007586832421366125\n",
      "Epoch :  134 Test loss :  -4.09913248538971 Test loss recursive :  0.007204869508277625\n",
      "Epoch :  135 Test loss :  -4.094823293685913 Test loss recursive :  0.0064300405676476656\n",
      "Epoch :  136 Test loss :  -4.0901553916931155 Test loss recursive :  0.00789826390799135\n",
      "Epoch :  137 Test loss :  -4.096510415077209 Test loss recursive :  0.006668618575204164\n",
      "Epoch :  138 Test loss :  -4.091448774337769 Test loss recursive :  0.008126574628986418\n",
      "Epoch :  139 Test loss :  -4.100231137275696 Test loss recursive :  0.006990092932246625\n",
      "Current probability of recursive training :  tensor(0.3140)\n",
      "Epoch :  140 Test loss :  -4.06210422039032 Test loss recursive :  0.008450938127934933\n",
      "Epoch :  141 Test loss :  -4.092020030021668 Test loss recursive :  0.006552388160489499\n",
      "Epoch :  142 Test loss :  -4.060249462127685 Test loss recursive :  0.0073781900340691205\n",
      "Epoch :  143 Test loss :  -4.103494520187378 Test loss recursive :  0.006398833505882067\n",
      "Epoch :  144 Test loss :  -4.064132924079895 Test loss recursive :  0.008523248796118424\n",
      "Epoch :  145 Test loss :  -4.039188451766968 Test loss recursive :  0.008404919307213276\n",
      "Epoch :  146 Test loss :  -4.0972188329696655 Test loss recursive :  0.006708614407107234\n",
      "Epoch :  147 Test loss :  -4.082360811233521 Test loss recursive :  0.007804282259894535\n",
      "Epoch :  148 Test loss :  -4.062919683456421 Test loss recursive :  0.008675349343102426\n",
      "Epoch :  149 Test loss :  -4.0933413457870484 Test loss recursive :  0.006504630756098777\n",
      "Current probability of recursive training :  tensor(0.3173)\n",
      "Epoch :  150 Test loss :  -4.097610921859741 Test loss recursive :  0.007440757411532104\n",
      "Epoch :  151 Test loss :  -4.099024844169617 Test loss recursive :  0.0070393886277452114\n",
      "Epoch :  152 Test loss :  -4.091814885139465 Test loss recursive :  0.007073274807189591\n",
      "Epoch :  153 Test loss :  -4.072997350692749 Test loss recursive :  0.007421037915628403\n",
      "Epoch :  154 Test loss :  -4.091412167549134 Test loss recursive :  0.006963195572025143\n",
      "Epoch :  155 Test loss :  -4.080015840530396 Test loss recursive :  0.009032091083936393\n",
      "Epoch :  156 Test loss :  -4.101114892959595 Test loss recursive :  0.006569839790463447\n",
      "Epoch :  157 Test loss :  -4.10292546749115 Test loss recursive :  0.006006125923449872\n",
      "Epoch :  158 Test loss :  -3.9737980270385744 Test loss recursive :  0.009890046556829474\n",
      "Epoch :  159 Test loss :  -4.098846569061279 Test loss recursive :  0.007674151258543134\n",
      "Current probability of recursive training :  tensor(0.3207)\n",
      "Epoch :  160 Test loss :  -4.068951358795166 Test loss recursive :  0.005217039399431087\n",
      "Epoch :  161 Test loss :  -4.079608850479126 Test loss recursive :  0.007976200715638697\n",
      "Epoch :  162 Test loss :  -4.041560740470886 Test loss recursive :  0.009440797208808362\n",
      "Epoch :  163 Test loss :  -4.101339111328125 Test loss recursive :  0.005767181851551868\n",
      "Epoch :  164 Test loss :  -4.098780269622803 Test loss recursive :  0.007533461931161583\n",
      "Epoch :  165 Test loss :  -4.072694396972656 Test loss recursive :  0.007795138725778088\n",
      "Epoch :  166 Test loss :  -4.091974835395813 Test loss recursive :  0.007076975898817181\n",
      "Epoch :  167 Test loss :  -4.039289011955261 Test loss recursive :  0.009666826510219835\n",
      "Epoch :  168 Test loss :  -4.099792437553406 Test loss recursive :  0.007246087852399797\n",
      "Epoch :  169 Test loss :  -4.071526627540589 Test loss recursive :  0.007877121393103153\n",
      "Current probability of recursive training :  tensor(0.3241)\n",
      "Epoch :  170 Test loss :  -3.918025941848755 Test loss recursive :  0.011900923512876034\n",
      "Epoch :  171 Test loss :  -4.1056951236724855 Test loss recursive :  0.0058881273842416705\n",
      "Epoch :  172 Test loss :  -4.072706804275513 Test loss recursive :  0.008059275480918586\n",
      "Epoch :  173 Test loss :  -4.097819356918335 Test loss recursive :  0.006931377304717898\n",
      "Epoch :  174 Test loss :  -4.07165678024292 Test loss recursive :  0.007093021330874762\n",
      "Epoch :  175 Test loss :  -4.04681134223938 Test loss recursive :  0.008410916050197557\n",
      "Epoch :  176 Test loss :  -4.0990926456451415 Test loss recursive :  0.006274199774488807\n",
      "Epoch :  177 Test loss :  -4.081961297988892 Test loss recursive :  0.008588757752440869\n",
      "Epoch :  178 Test loss :  -4.0996442317962645 Test loss recursive :  0.006651956143323332\n",
      "Epoch :  179 Test loss :  -4.098963580131531 Test loss recursive :  0.006801351749745663\n",
      "Current probability of recursive training :  tensor(0.3276)\n",
      "Epoch :  180 Test loss :  -4.088808875083924 Test loss recursive :  0.00906596179585904\n",
      "Epoch :  181 Test loss :  -4.103007774353028 Test loss recursive :  0.007708586980588734\n",
      "Epoch :  182 Test loss :  -4.102131118774414 Test loss recursive :  0.006194746675901115\n",
      "Epoch :  183 Test loss :  -4.103150939941406 Test loss recursive :  0.006964013241231442\n",
      "Epoch :  184 Test loss :  -4.094504747390747 Test loss recursive :  0.0074867961806012314\n",
      "Epoch :  185 Test loss :  -4.071569099426269 Test loss recursive :  0.007903095460496843\n",
      "Epoch :  186 Test loss :  -4.108065814971924 Test loss recursive :  0.006445337765617296\n",
      "Epoch :  187 Test loss :  -4.098050694465638 Test loss recursive :  0.006937689314363524\n",
      "Epoch :  188 Test loss :  -4.033987393379212 Test loss recursive :  0.006214804310584441\n",
      "Epoch :  189 Test loss :  -4.09841821193695 Test loss recursive :  0.006573497746139765\n",
      "Current probability of recursive training :  tensor(0.3310)\n",
      "Epoch :  190 Test loss :  -4.104478659629822 Test loss recursive :  0.007380010564811528\n",
      "Epoch :  191 Test loss :  -4.094858593940735 Test loss recursive :  0.007658331478014588\n",
      "Epoch :  192 Test loss :  -4.114950814247131 Test loss recursive :  0.007371090543456376\n",
      "Epoch :  193 Test loss :  -4.095232005119324 Test loss recursive :  0.006649221272091381\n",
      "Epoch :  194 Test loss :  -4.121418743133545 Test loss recursive :  0.007065676667261869\n",
      "Epoch :  195 Test loss :  -4.093524889945984 Test loss recursive :  0.008401011342357378\n",
      "Epoch :  196 Test loss :  -4.0571378898620605 Test loss recursive :  0.008015226077986881\n",
      "Epoch :  197 Test loss :  -4.090727734565735 Test loss recursive :  0.006969646255020052\n",
      "Epoch :  198 Test loss :  -4.098229098320007 Test loss recursive :  0.008724674540571868\n",
      "Epoch :  199 Test loss :  -4.104971799850464 Test loss recursive :  0.0077003503357991575\n",
      "Current probability of recursive training :  tensor(0.3345)\n",
      "Epoch :  200 Test loss :  -4.096938309669494 Test loss recursive :  0.00771219952381216\n",
      "Epoch :  201 Test loss :  -4.118093962669373 Test loss recursive :  0.008514057764550671\n",
      "Epoch :  202 Test loss :  -4.092923216819763 Test loss recursive :  0.005914103987743146\n",
      "Epoch :  203 Test loss :  -4.0879875183105465 Test loss recursive :  0.008288673548750012\n",
      "Epoch :  204 Test loss :  -4.07409257888794 Test loss recursive :  0.008382714004255831\n",
      "Epoch :  205 Test loss :  -4.105284838676453 Test loss recursive :  0.007391464343527332\n",
      "Epoch :  206 Test loss :  -4.09998420715332 Test loss recursive :  0.007922511477954686\n",
      "Epoch :  207 Test loss :  -4.092590136528015 Test loss recursive :  0.007911692312918604\n",
      "Epoch :  208 Test loss :  -4.110172257423401 Test loss recursive :  0.007160603180527687\n",
      "Epoch :  209 Test loss :  -4.115778722763062 Test loss recursive :  0.006500539670232683\n",
      "Current probability of recursive training :  tensor(0.3380)\n",
      "Epoch :  210 Test loss :  -4.049411926269531 Test loss recursive :  0.009877263740636409\n",
      "Epoch :  211 Test loss :  -4.092959094047546 Test loss recursive :  0.0070502536464482546\n",
      "Epoch :  212 Test loss :  -4.121779084205627 Test loss recursive :  0.006723452664446085\n",
      "Epoch :  213 Test loss :  -4.012279515266418 Test loss recursive :  0.011430866150185466\n",
      "Epoch :  214 Test loss :  -4.1182786560058595 Test loss recursive :  0.00661650410387665\n",
      "Epoch :  215 Test loss :  -4.11139714717865 Test loss recursive :  0.00619743668299634\n",
      "Epoch :  216 Test loss :  -4.147795057296753 Test loss recursive :  0.008769418126903475\n",
      "Epoch :  217 Test loss :  -4.121080451011657 Test loss recursive :  0.007374330330640078\n",
      "Epoch :  218 Test loss :  -4.178379311561584 Test loss recursive :  0.0076880834624171255\n",
      "Epoch :  219 Test loss :  -4.1965643548965454 Test loss recursive :  0.007993662129156292\n",
      "Current probability of recursive training :  tensor(0.3415)\n",
      "Epoch :  220 Test loss :  -4.200870876312256 Test loss recursive :  0.00850645664613694\n",
      "Epoch :  221 Test loss :  -4.261587691307068 Test loss recursive :  0.007258060546009801\n",
      "Epoch :  222 Test loss :  -3.980357217788696 Test loss recursive :  0.00846401342889294\n",
      "Epoch :  223 Test loss :  -4.311880779266358 Test loss recursive :  0.009250525869429111\n",
      "Epoch :  224 Test loss :  -4.138713364601135 Test loss recursive :  0.009903631587512791\n",
      "Epoch :  225 Test loss :  -4.0306990385055546 Test loss recursive :  0.006829310171306133\n",
      "Epoch :  226 Test loss :  -4.314563403129577 Test loss recursive :  0.01171721089631319\n",
      "Epoch :  227 Test loss :  -3.968648285865784 Test loss recursive :  0.009527570628561079\n",
      "Epoch :  228 Test loss :  -4.0580281019210815 Test loss recursive :  0.007687108013778925\n",
      "Epoch :  229 Test loss :  -4.013630409240722 Test loss recursive :  0.009246402855496853\n",
      "Current probability of recursive training :  tensor(0.3450)\n",
      "Epoch :  230 Test loss :  -4.083963389396668 Test loss recursive :  0.006840491921175271\n",
      "Epoch :  231 Test loss :  -4.388179507255554 Test loss recursive :  0.012873676959425212\n",
      "Epoch :  232 Test loss :  -4.26078016281128 Test loss recursive :  0.013201881907880306\n",
      "Epoch :  233 Test loss :  -4.119674863815308 Test loss recursive :  0.013709893715567887\n",
      "Epoch :  234 Test loss :  -4.075577025413513 Test loss recursive :  0.006048829945502803\n",
      "Epoch :  235 Test loss :  -4.105242466926574 Test loss recursive :  0.008218852446880191\n",
      "Epoch :  236 Test loss :  -4.095343480110168 Test loss recursive :  0.00904089444084093\n",
      "Epoch :  237 Test loss :  -4.115059266090393 Test loss recursive :  0.007728932460304349\n",
      "Epoch :  238 Test loss :  -3.9944119358062746 Test loss recursive :  0.00635076567879878\n",
      "Epoch :  239 Test loss :  -4.104807004928589 Test loss recursive :  0.006841714598413091\n",
      "Current probability of recursive training :  tensor(0.3485)\n",
      "Epoch :  240 Test loss :  -4.125412921905518 Test loss recursive :  0.007643741278443485\n",
      "Current loss : 0.00, ... 46, / 1000, Current memory usage : 2305 MB, loaded 1000     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m scheduler\u001b[39m.\u001b[39mstep(\u001b[39mlen\u001b[39m(losses) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m start(model, optimizer, scheduler, data_train, data_test, device, \\\n\u001b[0;32m      7\u001b[0m         epochs, \u001b[39mlen\u001b[39;49m(losses) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m, grapher\u001b[39m=\u001b[39;49mgrapher, save\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, save_datasets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(model, optimizer, scheduler, data_train, data_test, device, epoch, offset, grapher, save, save_datasets)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(offset, offset \u001b[39m+\u001b[39m epoch):\n\u001b[0;32m      5\u001b[0m     recursive \u001b[39m=\u001b[39m e \u001b[39m>\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     model \u001b[39m=\u001b[39m train(model, optimizer, scheduler, data_train, device, e, process, max_epoch\u001b[39m=\u001b[39;49moffset\u001b[39m+\u001b[39;49mepoch, recursive\u001b[39m=\u001b[39;49mrecursive)\n\u001b[0;32m      9\u001b[0m     \u001b[39m#model.show_gradients()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m(e \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m save_datasets) :\n",
      "File \u001b[1;32mc:\\Users\\gille\\Desktop\\graph-displacement\\cell_training.py:213\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, data, device, epoch, process, max_epoch, recursive)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent loss : \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m, ... \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, / \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Current memory usage : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m MB, loaded \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m    \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(loss\u001b[39m.\u001b[39mitem(), i, data\u001b[39m.\u001b[39mlen(), process\u001b[39m.\u001b[39mmemory_info()\u001b[39m.\u001b[39mrss \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m1000000\u001b[39m, \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mmemory)), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)  \u001b[39m# in megabytes\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 213\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    215\u001b[0m     scheduler\u001b[39m.\u001b[39mstep((epoch \u001b[39m+\u001b[39m i) \u001b[39m/\u001b[39m data\u001b[39m.\u001b[39mlen()) \u001b[39m#type: ignore\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\gille\\miniconda3\\envs\\geom\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gille\\miniconda3\\envs\\geom\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gille\\miniconda3\\envs\\geom\\lib\\site-packages\\adamp\\adamp.py:91\u001b[0m, in \u001b[0;36mAdamP.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     89\u001b[0m wd_ratio \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(p\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     perturb, wd_ratio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_projection(p, grad, perturb, group[\u001b[39m'\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m'\u001b[39;49m], group[\u001b[39m'\u001b[39;49m\u001b[39mwd_ratio\u001b[39;49m\u001b[39m'\u001b[39;49m], group[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     93\u001b[0m \u001b[39m# Weight decay\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 630\n",
    "grapher = GraphingLoss(losses)\n",
    "scheduler.step(len(losses) // 2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "start(model, optimizer, scheduler, data_train, data_test, device, \\\n",
    "        epochs, len(losses) // 2, grapher=grapher, save=10, save_datasets=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
