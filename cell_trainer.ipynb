{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will use torch_geometric to predict the developpement of a graph of positions through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI realized I am leaning towards this approach https://doi.org/10.1016/j.trc.2020.102635\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I realized I am leaning towards this approach https://doi.org/10.1016/j.trc.2020.102635\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  361\n",
      "Available :  3475\n",
      "True\n",
      "Cannot import zuko. Continuing without prior\n",
      "No module named 'pycapmd'\n",
      "Cannot import simulator\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from genericpath import exists\n",
    "\n",
    "model_path = \"models/model\"\n",
    "loss_path = \"models/loss\"\n",
    "\n",
    "from cell_dataset import CellGraphDataset\n",
    "from cell_model import GraphEvolution\n",
    "from cell_utils import GraphingLoss\n",
    "from cell_training import train, test_single, test_recursive, run_single, run_single_recursive\n",
    "\n",
    "import os, psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes \n",
    "print(\"Available : \", process.memory_info().vms  // 1000000)  # in megabytes \n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "#https://github.com/clovaai/AdamP\n",
    "from adamp import AdamP\n",
    "\n",
    "sys.path.append('/home/nstillman/1_sbi_activematter/cpp_model')\n",
    "try :\n",
    "    import allium\n",
    "except :\n",
    "    print(\"Could not import allium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a graph of cells having their own positions and velocity.\n",
    "\n",
    "In the graph, we will first start by connecting all the edges, then maybe later make radius_graphs to reduce the cost of the pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all = True\n",
    "\n",
    "if load_all : \n",
    "    if os.path.exists(\"data/training.pkl\") :\n",
    "        with open(\"data/training.pkl\", \"rb\") as f:\n",
    "            data_train = pickle.load(f)\n",
    "    if os.path.exists(\"data/testing.pkl\") :\n",
    "        with open(\"data/testing.pkl\", \"rb\") as f:\n",
    "            data_test = pickle.load(f)\n",
    "else :\n",
    "    #path = \"data/\" #local\n",
    "    path = \"/scratch/users/nstillman/data-cpp/\" #remote\n",
    "\n",
    "    data_train = CellGraphDataset(root=path + 'train', max_size=1000, rdts=True, inmemory=True, bg_load=True, wrap=True, T_limit=16)\n",
    "    print(\"Training data length : \", data_train.len())\n",
    "\n",
    "    data_test = CellGraphDataset(root=path + 'test', max_size=50, inmemory=True, bg_load=True, wrap=True, T_limit=16)\n",
    "    print(\"Test data length : \", data_test.len())\n",
    "    \n",
    "    data_val = CellGraphDataset(root=path + 'valid', max_size=50, inmemory=True, bg_load=True, wrap=True, T_limit=8)\n",
    "    print(\"Validation data length : \", data_val.len())\n",
    "\n",
    "    override = True #make this true to always use the same ones\n",
    "\n",
    "    if override :\n",
    "        data_train.save_or_load_if_exists(\"train_paths.pkl\")\n",
    "        data_test.save_or_load_if_exists(\"test_paths.pkl\")\n",
    "        data_val.save_or_load_if_exists(\"val_paths.pkl\")\n",
    "    else :\n",
    "        torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO : if bg_load is True, this starts the loading, if skipped, bg_loading will take place as soon as a get is called\n",
    "rval, edge_index, edge_attr, batch_edge, border, params = data_train.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the model that will be used :\n",
    "    > input \n",
    "        (1) Graph at a particular time t (nodes having x,y,dx,dy as attributes)\n",
    "        (2) Graphs up to a particular time [t-a, t] (nodes having x,y as attributes)\n",
    "    > output\n",
    "        (a) Graph at the immediate next time step t+1\n",
    "        (b) Graph [t, t+b]\n",
    "        (c) Graph at t+b\n",
    "    > graph size\n",
    "        (x) Fixed graph size to the most nodes possible (or above)\n",
    "        (y) Unbounded graph size\n",
    "            >> idea : graph walks\n",
    "            >> idea : sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model will do (1ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global losses\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(model : GraphEvolution, optimizer : torch.optim.Optimizer, scheduler  : torch.optim.lr_scheduler._LRScheduler,\\\n",
    "          data_train : CellGraphDataset, data_test : CellGraphDataset, device : torch.device, epoch : int, offset : int, save=0, save_datasets=True):\n",
    "    for e in range(offset, offset + epoch):\n",
    "        \n",
    "        recursive = True\n",
    "\n",
    "        model = train(model, optimizer, scheduler, data_train, device, e, process, max_epoch=offset+epoch, recursive=recursive)\n",
    "\n",
    "        test_loss_s = test_single(model, data_test, device, duration=8)\n",
    "        test_loss_r = test_recursive(model, data_test, device, duration=8)\n",
    "        \n",
    "        if(e == 0 and save_datasets) :\n",
    "            with open(\"data/training.pkl\", 'wb') as f:\n",
    "                pickle.dump(data_train, f)\n",
    "            with open(\"data/testing.pkl\", 'wb') as f:\n",
    "                pickle.dump(data_test, f)\n",
    "            print(\"Saved datasets\")\n",
    "        \n",
    "        if (e%10 == 0) :\n",
    "            print(\"Epoch : \", e, \"Test loss : \", test_loss_s, \"Test loss recursive : \", test_loss_r)\n",
    "        \n",
    "        losses.append(test_loss_r)\n",
    "        losses.append(test_loss_s)\n",
    "        \n",
    "        if (save and (e%save == 0 or e == epoch-1)) :\n",
    "            torch.save(model.state_dict(), model_path + str(e) + \".pt\")\n",
    "            with open(loss_path + str(e) + \".pkl\", 'wb') as f:\n",
    "                pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "\n",
    "epoch_to_load = 225\n",
    "\n",
    "model = GraphEvolution(in_channels=9, out_channels=4, hidden_channels=32, dropout=0.01, edge_dim=2, messages=5)\n",
    "\n",
    "if exists(model_path + str(epoch_to_load) + \".pt\") and load :\n",
    "    with open(loss_path + str(epoch_to_load) + \".pkl\", 'rb') as f:\n",
    "        losses = pickle.load(f)\n",
    "    model.load_state_dict(torch.load(model_path + str(epoch_to_load) + \".pt\"))\n",
    "    print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  3436\n",
      "Losses :  0\n",
      "Model :  GraphEvolution(\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "        (dropout3): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.01, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.01, inplace=False)\n",
      "        (dropout2): Dropout(p=0.01, inplace=False)\n",
      "        (dropout3): Dropout(p=0.01, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_resize): Conv1d(32, 4, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Using : \", process.memory_info().rss // 1000000)  # in megabytes\n",
    "print(\"Losses : \", len(losses) // 2)\n",
    "print(\"Model : \", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#might want to investigate AdamP \n",
    "optimizer = AdamP(model.parameters(), lr=5e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=5e-3, delta=0.1, wd_ratio=0.1, nesterov=True)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10, T_mult=2, eta_min=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current probability of recursive training :  tensor(0.1080)\n",
      "Epoch :  0 Test loss :  0.07798135828226804 Test loss recursive :  1.260199143886566 \n",
      "Current loss : 1.08, ... 278, / 1000, Current memory usage : 4774 MB, loaded 1000    \r"
     ]
    }
   ],
   "source": [
    "epochs = 630\n",
    "grapher = GraphingLoss(losses)\n",
    "scheduler.step(len(losses) // 2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try :\n",
    "    grapher.gstart(20)\n",
    "    start(model, optimizer, scheduler, data_train, data_test, device, \\\n",
    "          epochs, len(losses) // 2, save=10, save_datasets=False)\n",
    "finally :\n",
    "    grapher.gstop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geom_ff]",
   "language": "python",
   "name": "conda-env-geom_ff-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
