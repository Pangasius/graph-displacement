{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_open_ht_hv_2_64_relative_laplace\n",
      "models/result_recursive_open_ht_hv_2_64_relative_laplace_20_0\n"
     ]
    }
   ],
   "source": [
    "load_all =  True #load directly from a pickle\n",
    "pre_separated = False #if three subfolders already exist for train test and val\n",
    "\n",
    "override = False #make this true to always use the same ones\n",
    "\n",
    "extension = \"_open_ht_hv\"\n",
    "number_of_messages = 2\n",
    "size_of_messages = 64\n",
    "absolute = 0\n",
    "epochs = 81\n",
    "distrib = \"laplace\"\n",
    "\n",
    "name_complete = extension + \"_\" + str(number_of_messages) + \"_\" + str(size_of_messages) + \"_\" + (\"absolute\" if absolute else \"relative\") + \"_\" + distrib\n",
    "\n",
    "print(name_complete)\n",
    "\n",
    "model_path = \"models/model\" + name_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_dataset import load\n",
    "data_train, data_test, data_val = load(load_all, extension, pre_separated, override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_model import Gatv2Predictor\n",
    "from genericpath import exists\n",
    "import torch\n",
    "\n",
    "epoch_to_load = 0\n",
    "\n",
    "model = Gatv2Predictor(in_channels=12, out_channels=8, hidden_channels=size_of_messages, dropout=0.1, edge_dim=2, messages=number_of_messages, wrap=data_train.wrap, absolute=absolute)\n",
    "\n",
    "if exists(model_path + str(epoch_to_load) + \".pt\") :\n",
    "    model.load_state_dict(torch.load(model_path + str(epoch_to_load) + \".pt\"))\n",
    "    print(\"Loaded model\")\n",
    "else :\n",
    "    raise Exception(\"Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_training import predict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_y, data_x = predict(model, data_test, device, -1, recursive=True, distrib=distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we skip the first of the data because it has 0 speed and it makes the plot crash (1/0)\n",
    "data_y = data_y[:,1:] #model\n",
    "data_x = data_x[:,1:] #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allium import SimData\n",
    "import numpy as np\n",
    "\n",
    "class Parameters(object):\n",
    "    def __init__(self, p):\n",
    "        for key, values in p.items():\n",
    "            setattr(self, key, values)\n",
    "\n",
    "class SyntheticData:\n",
    "    @staticmethod\n",
    "    def checkTypes(readtypes,data):\n",
    "        #check which particles to load \n",
    "        if len(readtypes) > 0:\n",
    "            usetypes = np.isin(data[:,-1],readtypes)\n",
    "        else:\n",
    "            usetypes = [True]*len(data)\n",
    "        return usetypes\n",
    "\n",
    "    # Data object for summary statistics\n",
    "    def __init__(self,**kwargs):\n",
    "        # check for debugging\n",
    "        try:\n",
    "            self.debug = kwargs['debug']\n",
    "            if self.debug:\n",
    "                print('kwargs: ', kwargs)\n",
    "        except:\n",
    "            self.debug = False\n",
    "        # check for specific loadtimes\n",
    "        try:    \n",
    "            self.start = kwargs[\"loadtimes\"][0]\n",
    "            self.end = kwargs[\"loadtimes\"][1]\n",
    "            self.multiopt = True\n",
    "        except:\n",
    "            self.multiopt = False\n",
    "        # check for specific types\n",
    "        try:\n",
    "            self.readtypes = kwargs[\"readtypes\"]\n",
    "        except:\n",
    "            self.readtypes = []\n",
    "            \n",
    "        try:\n",
    "            self.dt = kwargs[\"dt\"]\n",
    "        except:\n",
    "            self.dt = 1\n",
    "        # load parameters\n",
    "        try:    \n",
    "            self.param = Parameters(kwargs['params'])\n",
    "        except Exception as e:\n",
    "            print('Error! Parameters must be a dictionary', e)\n",
    "            return 1\n",
    "        # load multiple simulation snapshots\n",
    "        self.Nsnap = self.end - self.start + 1\n",
    "        #get maximum number of particles\n",
    "        self.N = sum(SimData.checkTypes(self.readtypes, kwargs['data'][0]))\n",
    "        self.Nvals = []\n",
    "        self.Nvariable =  False\n",
    "        for t in range(self.start,self.end):\n",
    "            self.Nvals.append(sum(SimData.checkTypes(self.readtypes, kwargs['data'][t])))\n",
    "            if self.Nvals[t] > self.N:\n",
    "                self.N = self.Nvals[t] \n",
    "                self.Nvariable = True\n",
    "        if kwargs['trackAll'] and self.Nvariable == False:\n",
    "            self.Ntracers = self.Nvals[0]\n",
    "        else:\n",
    "            print('Error! Currently assuming tracking all cells')\n",
    "        self.data = kwargs['data']\n",
    "        print(self.data.shape[-1])\n",
    "        self.flag = np.zeros((self.Nsnap,self.N))\n",
    "        if self.data.shape[-1] == 5:\n",
    "            self.Z = self.data[:,:,4]\n",
    "        else:\n",
    "            np.zeros((self.Nsnap,self.N))\n",
    "            \n",
    "        self.rval = self.data[:,:,:2]\n",
    "        self.vval = self.data[:,:,2:4]\n",
    "        self.theta = np.zeros((self.Nsnap,self.N))\n",
    "        self.nval = np.zeros((self.Nsnap,self.N,2))\n",
    "        self.radius = np.ones((self.Nsnap,self.N))\n",
    "        self.ptype = np.ones((self.Nsnap,self.N))\n",
    "        self.sigma = 0.\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def gettypes(self, readtypes, frames):\n",
    "        return np.isin(self.ptype[frames],readtypes)\n",
    "\n",
    "    def truncateto(self,start, endtime):\n",
    "        self.Nsnap = endtime - start\n",
    "        self.flag =  self.flag[start:endtime]\n",
    "        self.rval = self.rval[start:endtime]\n",
    "        self.vval = self.vval[start:endtime]\n",
    "        self.theta = self.theta[start:endtime]\n",
    "        self.nval = self.nval[start:endtime]\n",
    "        self.radius = self.radius[start:endtime]\n",
    "        self.ptype = self.ptype[start:endtime]\n",
    "        self.Nvals = self.Nvals[start:endtime]\n",
    "        self.Ntracers = self.Ntracers[start:endtime]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import allium.summstats as ss\n",
    "\n",
    "configfile = \"simconfig_open.json\"\n",
    "\n",
    "with open(configfile) as jsonFile:\n",
    "    parameters = json.load(jsonFile)\n",
    "    \n",
    "vav_x = []\n",
    "vdist_x = []\n",
    "vdist2_x = []\n",
    "\n",
    "vav_y = []\n",
    "vdist_y = []\n",
    "vdist2_y = []\n",
    "\n",
    "msd_x = []\n",
    "msd_y = []\n",
    "    \n",
    "velbins=np.linspace(0,5,100)\n",
    "velbins2=np.linspace(-2,2,100)\n",
    "\n",
    "    \n",
    "for i in range(data_x.shape[0]):\n",
    "    data_x_i = data_x[i]\n",
    "    data_y_i = data_y[i]\n",
    "    \n",
    "    data_x_i_s = SyntheticData(loadtimes = [0,96], types = [0,1], debug = False, data = data_x_i, params = parameters, trackAll=True, dt = 1)\n",
    "    data_y_i_s = SyntheticData(loadtimes = [0,96], types = [0,1], debug = False, data = data_y_i, params = parameters, trackAll=True, dt = 1)\n",
    "    \n",
    "    Nsnap  = data_x_i_s.Nsnap\n",
    "    dt = data_x_i_s.param.dt\n",
    "    output_time = data_x_i_s.param.output_time\n",
    "\n",
    "    vav, vdist,vdist2 = ss.getVelDist(data_x_i_s, velbins,velbins2, usetype=[0,1],verbose=False)\n",
    "\n",
    "    vdist = vdist[1:]\n",
    "    vdist2 = vdist2[vdist2 != max(vdist2)]\n",
    "    \n",
    "    vav_x.append(vav)\n",
    "    vdist_x.append(vdist)\n",
    "    vdist2_x.append(vdist2)\n",
    "    \n",
    "    velbins=np.linspace(0,5,100)\n",
    "    velbins2=np.linspace(-2,2,100)\n",
    "    vav, vdist,vdist2 = ss.getVelDist(data_y_i_s, velbins,velbins2, usetype=[0,1],verbose=False)\n",
    "\n",
    "    vdist = vdist[1:]\n",
    "    vdist2 = vdist2[vdist2 != max(vdist2)]\n",
    "    \n",
    "    vav_y.append(vav)\n",
    "    vdist_y.append(vdist)\n",
    "    vdist2_y.append(vdist2)\n",
    "    \n",
    "    tval, msd, d = ss.getMSD(data_x_i_s, usetype=[1],verbose=False)\n",
    "    \n",
    "    msd_x.append(msd)\n",
    "    \n",
    "    tval, msd, d = ss.getMSD(data_y_i_s, usetype=[1],verbose=False)\n",
    "    \n",
    "    msd_y.append(msd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vav_x = np.array(vav_x)\n",
    "vdist_x = np.array(vdist_x)\n",
    "vdist2_x = np.array(vdist2_x)\n",
    "\n",
    "vav_y = np.array(vav_y)\n",
    "vdist_y = np.array(vdist_y)\n",
    "vdist2_y = np.array(vdist2_y)\n",
    "\n",
    "msd_x = np.array(msd_x)\n",
    "msd_y = np.array(msd_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "vdist_x_mean = np.mean(vdist_x, axis=0)\n",
    "vdist_x_std = np.std(vdist_x, axis=0)\n",
    "\n",
    "vdist2_x_mean = np.mean(vdist2_x, axis=0)\n",
    "vdist2_x_std = np.std(vdist2_x, axis=0)\n",
    "\n",
    "vav_x_mean = np.mean(vav_x, axis=0)\n",
    "vav_x_std = np.std(vav_x, axis=0)\n",
    "\n",
    "vdist_y_mean = np.mean(vdist_y, ayis=0)\n",
    "vdist_y_std = np.std(vdist_y, axis=0)\n",
    "\n",
    "vdist2_y_mean = np.mean(vdist2_y, axis=0)\n",
    "vdist2_y_std = np.std(vdist2_y, axis=0)\n",
    "\n",
    "vav_y_mean = np.mean(vav_y, axis=0)\n",
    "vav_y_std = np.std(vav_y, axis=0)\n",
    "\n",
    "msd_x_mean = np.mean(msd_x, axis=0)\n",
    "msd_x_std = np.std(msd_x, axis=0)\n",
    "\n",
    "msd_y_mean = np.mean(msd_y, axis=0)\n",
    "msd_y_std = np.std(msd_y, axis=0)\n",
    "\n",
    "fig=plt.figure()\n",
    "db=velbins[1]-velbins[0]\n",
    "plt.semilogy(velbins[1:]-db/2,vdist_x_mean,'r.-',lw=2, label='Data')\n",
    "plt.fill_between(velbins[1:]-db/2,vdist_x_mean+vdist_x_std,vdist_x_mean-vdist_x_std,color='r',alpha=0.2)\n",
    "plt.semilogx(velbins[1:]-db/2,vdist2_y_mean,'b.-',lw=2, label='Model')\n",
    "plt.fill_between(velbins[1:]-db/2,vdist2_y_mean+vdist2_y_std,vdist2_y_mean-vdist2_y_std,color='b',alpha=0.2)\n",
    "plt.xlabel('v/<v>')\n",
    "plt.ylabel('P(v/<v>)')\n",
    "plt.title('Scaled velocity magnitude distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('svmd' + name_complete + '.pdf')\n",
    "\n",
    "fig=plt.figure()\n",
    "db=velbins2[1]-velbins2[0]\n",
    "plt.semilogy(velbins2[1:]-db/2,vdist2_x_mean,'r.-',lw=2, label='Data')\n",
    "plt.fill_between(velbins2[1:]-db/2,vdist2_x_mean+vdist2_x_std,vdist2_x_mean-vdist2_x_std,color='r',alpha=0.2)\n",
    "plt.semilogx(velbins2[1:]-db/2,vdist2_y_mean,'b.-',lw=2, label='Model')\n",
    "plt.fill_between(velbins2[1:]-db/2,vdist2_y_mean+vdist2_y_std,vdist2_y_mean-vdist2_y_std,color='b',alpha=0.2)\n",
    "plt.xlabel('v/<v>')\n",
    "plt.ylabel('P(v/<v>)')\n",
    "plt.title('Scaled velocity component (x & y) distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('svcd' + name_complete + '.pdf')\n",
    "\n",
    "fig = plt.figure()\n",
    "xval=np.linspace(0,(Nsnap-1)*dt*output_time,num=Nsnap-1)\n",
    "plt.plot(xval,vav_x_mean,'r.-',lw=2, label='Data')\n",
    "plt.fill_between(xval,vav_x_mean+vav_x_std,vav_x_mean-vav_x_std,color='r',alpha=0.2)\n",
    "plt.plot(xval,vav_y_mean,'b.-',lw=2, label='Model')\n",
    "plt.fill_between(xval,vav_y_mean+vav_y_std,vav_y_mean-vav_y_std,color='b',alpha=0.2)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('mean velocity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('mv' + name_complete + '.pdf')\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.loglog(tval,msd_x_mean,'r.-',lw=2, label='Data')\n",
    "plt.loglog(tval,msd_x_mean[1]/(1.0*tval[1])*tval,'--',lw=2,color=\"orange\")\n",
    "plt.loglog(tval,msd_y_mean,'b.-',lw=2, label='Model')\n",
    "plt.loglog(tval,msd_y_mean[1]/(1.0*tval[1])*tval,'--',lw=2,color=\"cyan\")\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('MSD')\n",
    "plt.title('Mean square displacement')\n",
    "plt.show()\n",
    "fig.savefig('msd' + name_complete + '.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
